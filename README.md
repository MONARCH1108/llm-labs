# ðŸ“ LLM Labs - Directory Structure


```
LLM_Labs_V1/
â”‚
â”œâ”€ comparision_tools/
â”‚   â”œâ”€ metrics.py
â”‚   â”‚   â”œâ”€ get_text_stats(text: str) -> dict
â”‚   â”‚   â”œâ”€ compute_flesch_reading_ease(sentence_count: int, word_count: int, syllable_count: int) -> float|str
â”‚   â”‚   â”œâ”€ compute_smog_index(sentence_count: int, polysyllable_count: int) -> float|str
â”‚   â”‚   â”œâ”€ compute_coleman_liau_index(char_count: int, word_count: int, sentence_count: int) -> float|str
â”‚   â”‚   â”œâ”€ compute_gunning_fog_index(sentence_count: int, word_count: int, polysyllable_count: int) -> float|str
â”‚   â”‚   â”œâ”€ compute_automated_readability_index(char_count: int, word_count: int, sentence_count: int) -> float|str
â”‚   â”‚   â”œâ”€ compute_dale_chall_index(text: str) -> float
â”‚   â”‚   â”œâ”€ compute_forcast_index(text: str) -> float
â”‚   â”‚   â”œâ”€ compute_linsear_write_index(text: str) -> float
â”‚   â”‚   â”œâ”€ compute_lix(text: str) -> float
â”‚   â”‚   â”œâ”€ compute_rix(text: str) -> float
â”‚   â”‚   â””â”€ get_readability_metrics(text: str) -> dict
â”‚   â”‚
â”‚   â”œâ”€ prompt_classifier.py
â”‚   â”‚   â””â”€ classify_prompt(prompt: str) -> str
â”‚   â”‚
â”‚   â””â”€ tokenizer.py
â”‚       â””â”€ count_tokens(text: str) -> int
â”‚
â”œâ”€ utils/
â”‚   â”œâ”€ comparison.py
â”‚   â”‚   â””â”€ run_comparative_evaluation(providers_models: dict, prompts: dict, user_input: str)
â”‚   â”‚
â”‚   â”œâ”€ llms.py
â”‚   â”‚   â”œâ”€ query_groq_llm(user_input: str, model: str, prompt: str) -> str
â”‚   â”‚   â”œâ”€ query_gemini_llm(user_input: str, model: str, prompt: str) -> str
â”‚   â”‚   â””â”€ query_ollama_llm(user_input: str, model: str, prompt: str) -> str
â”‚   â”‚
â”‚   â”œâ”€ prompts.py
â”‚   â”‚   â”œâ”€ zero_shot_prompt() -> str
â”‚   â”‚   â”œâ”€ one_shot_prompt() -> str
â”‚   â”‚   â”œâ”€ few_shot_prompt() -> str
â”‚   â”‚   â”œâ”€ chain_of_thought_prompt() -> str
â”‚   â”‚   â”œâ”€ react_prompt() -> str
â”‚   â”‚   â”œâ”€ self_ask_prompt() -> str
â”‚   â”‚   â”œâ”€ tree_of_thought_prompt() -> str
â”‚   â”‚   â”œâ”€ instruction_constraints_prompt() -> str
â”‚   â”‚   â””â”€ persona_based_prompt_template(role: str, tone: str = "professional", style: str = "clear and concise") -> str
â”‚   â”‚
â”‚   â””â”€ report_generator.py
â”‚       â”œâ”€ safe_float(value: str)
â”‚       â”œâ”€ extract_log_metrics() -> list
â”‚       â””â”€ generate_report(summary: list, user_question: str)
â”‚
â””â”€ app.py
    â”œâ”€ select_prompt_templates() -> dict
    â”œâ”€ run_comparative()
    â”œâ”€ run_single_llm_chat()
    â””â”€ main()
```

## Key Corrections from Your Original Format:

### 1. **File Organization**:

- `metrics.py` contains all readability functions (not in `tokenizer.py`)
- `llms.py` contains all LLM query functions (separate from `comparison.py`)
- `prompts.py` contains all prompt templates (separate from `comparison.py`)

### 2. **Function Distribution**:

- **`metrics.py`**: 12 functions for text analysis and readability
- **`tokenizer.py`**: Only 1 function for token counting
- **`llms.py`**: 3 LLM provider functions
- **`prompts.py`**: 9 prompting strategy functions
- **`comparison.py`**: 1 main orchestration function
- **`report_generator.py`**: 3 functions for report generation

### 3. **Missing Functions Added**:

- `safe_float()` in `report_generator.py`
- All individual readability metric functions in `metrics.py`
- Proper parameter types and return types specified

### 4. **Accurate File Count**:

- **Total Files**: 7 Python files
- **Total Functions**: 30 functions
- **Modules**: 2 main directories (`comparision_tools/`, `utils/`)